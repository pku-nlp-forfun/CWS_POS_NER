\section{实验内容}
\label{sec:experiment}

最一开始当然是从观察数据开始，并且先将数据处理成可以训练的模式。

我们主要处理三阶段的可训练数据。
第一个就是处理成分词用的训练数据，将数据转换为针对每一个字的四分类问题，在段落~\ref{sec:purpose}也提过，我们所预测的集合为\{B, M, E, S\}。字的部份我们就先不做处理，保留之后转成各种不同的 embedding 表示。
第二阶段数据则是给词性标注的数据，毕竟我们是两个任务并行进行的，但词性标注必须依赖先有分词的结果，故我们将词性标注的 gold 数据处理后形成模拟的分词结果（不直接使用分词的 gold 数据处理，主要是因为分词的数据连<sub> <sup>之类的 tag 都被搞坏了，根本没办法用）。
第三阶段数据是在最终的文本数据提供之前，先利用现有数据还原成原始文章的形式，这是用于测试从无到有的模型输出效果，之后待最后的目标文档提供后只要直接替换这边就可以快速的套用并输出结果。

再来，则是模型的搭建与训练和测试，这部份在前面的段落~\ref{sec:principle}中有详细的叙述。

最后是模型的评价，原先使用了所提供的评价脚本，却发现里面有很多的坑，特别是因为数据中的"//w"和"</sup>"这类数据会被误判成词性，还有空白行也会一同被纳入评测，这导致最后评分出来的表现有很大的误差，故为了保险起见还是针对了这部份进行我们自己版本的改正。

\subsection*{CWS}

在分词过程中，测试了CRF单模型，Embed使用了One-Hot, TF\_IDF, FastText.

因为在测试过程中发现用预训练词向量效果不好，就没有测试基于Bert或者ELMo的实验。

在跑单CRF过程中，因为输入的是一个句子数*句子长度*特征数的一个三维矩阵。
当利用最大句子长度对齐时，在特征数较多的情况下（即One-Hot）会导致Memory Error。
例如训练集6106句，最大长度1060，One-Hot2868维度。用int16存储，需要68GB内存，实际测试中用了int32内存直接打到120GB+。
故在处理这个情况下， 使用了reshape.

另外，在分词这个任务，除了最基本的MASK操作之外，实际上带有一个隐藏的条件，即句尾一定分词。
虽然为了学习到上下文信息，不易直接将句尾字符舍去，但在评测过程中，需要手动更改。

因为使用单模型，效果较为一般，为了获得更多上下文信息，在encoder层按常规接了一个BiLSTM。

实验下来，效果比较稳定。像单模型CRF，训练集和开发集偏差有将近25\%, 而BiLSTM-CRF模型偏差在5\%以内。