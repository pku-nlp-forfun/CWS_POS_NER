\section{误差分析}
\label{sec:analysis}

一开始花了挺多时间在检查F1 score一致性问题上，一致性主要要注意MASK，还有句尾一定分词这个隐藏的条件，另外给的评测脚本和自己写的f1还是有些偏差，但无伤大雅。

到最后开始调模型的时候发现单模型CRF tf-idf效果竟然比One HOT还差。

TF IDF的思路是先做一个窗口获得附近centre word 附近n gram词汇信息，做相应的词频转换。然后丢到sklearn的pipeline中做tf-idf的转换。

因为TF IDF 测试集需要和训练集对齐，在这个处理上把未出现的词都塞到最后一个word的dict中，赋予所有词频为0.

所以一开始怀疑是不是训练集和开发集对齐发生错误，毕竟33\%左右的正确率基本上意味着random。不过因为时间关系，后面就没去check了。

同样替换embed 为常见的预训练词向量，比如说FastText，Glove， ELMo， Bert。

本来想把实验做做完，但做FastText下来效果很差，就直接改BiLSTM-CRF了。

总的来说，在实验过程中TF-IDF训练效果明显低于正常值，且其训练集基本上达到最优效果，很有可能是测试集合训练集特征为对应上。
