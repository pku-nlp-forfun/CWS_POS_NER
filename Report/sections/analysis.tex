\section{误差分析}
\label{sec:analysis}

一开始花了挺多时间在检查F1 score一致性问题上，一致性主要要注意MASK，还有句尾一定分词这个隐藏的条件，另外给的评测脚本和自己写的 F1 score 还是有些偏差，但无伤大雅。

到最后开始调模型的时候发现单模型CRF TF-IDF 效果竟然比 One-Hot还差。

TF-IDF 的思路是先做一个窗口获得附近centre word 附近 n-gram 词汇信息，做相应的词频转换。然后丢到 sklearn 的 pipeline 中做 TF-IDF 的转换。

因为 TF-IDF 测试集需要和训练集对齐，在这个处理上把未出现的词都塞到最后一个 word 的 dict 中，赋予所有词频为0.

所以一开始怀疑是不是训练集和开发集对齐发生错误，毕竟33\%左右的正确率基本上意味着random。不过因为时间关系，后面就没去check了。

同样替换 embedding 为常见的预训练词向量，比如说FastText，GloVe， ELMo， BERT。

本来想把实验做做完，但做FastText下来效果很差，就直接改BiLSTM-CRF了。

总的来说，在实验过程中 TF-IDF 训练效果明显低于正常值，且其训练集基本上达到最优效果，很有可能是测试集合训练集特征为对应上。
